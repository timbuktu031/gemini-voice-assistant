# modules/gemini_client.py
import google.generativeai as genai
from config import api_keys, app_config
import time
import re

class GeminiClient:
    """Gemini API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        # API í‚¤ ì„¤ì • (config.pyì—ì„œ ì´ë¯¸ ì„¤ì •ë¨)
        self.model_name = "gemini-2.0-flash-exp"
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        try:
            self.model = genai.GenerativeModel(self.model_name)
            print(f"âœ… Gemini ëª¨ë¸ ({self.model_name}) ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.model = None
        
        # ìƒì„± ì„¤ì •
        self.generation_config = {
            "temperature": 0.7,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 2048,
        }
        
        # ì•ˆì „ ì„¤ì •
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]
    
    def generate_response(self, prompt, context="", max_retries=3):
        """ì‘ë‹µ ìƒì„±"""
        if not self.model:
            return "Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = self._build_full_prompt(prompt, context)
        
        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(
                    full_prompt,
                    generation_config=self.generation_config,
                    safety_settings=self.safety_settings
                )
                
                # ì‘ë‹µ ê²€ì¦
                if response and response.text:
                    return self._post_process_response(response.text)
                else:
                    print(f"âš ï¸  ë¹ˆ ì‘ë‹µ (ì‹œë„ {attempt + 1}/{max_retries})")
                    
            except Exception as e:
                print(f"âŒ Gemini API ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                else:
                    return f"ì£„ì†¡í•©ë‹ˆë‹¤. {max_retries}ë²ˆ ì‹œë„ í›„ì—ë„ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _build_full_prompt(self, prompt, context):
        """ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ìŒì„± AI ë¹„ì„œì…ë‹ˆë‹¤. ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ìŒì„±ìœ¼ë¡œ ë“¤ì—ˆì„ ë•Œ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”
3. ê°€ëŠ¥í•œ ê°„ê²°í•˜ë˜ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”
5. ì‹¤ì‹œê°„ ì •ë³´ê°€ ì œê³µëœ ê²½ìš° ì´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”

"""
        
        full_prompt = system_prompt
        
        if context:
            full_prompt += f"{context}\n"
        
        full_prompt += f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}\n\në‹µë³€:"
        
        return full_prompt
    
    def _post_process_response(self, response_text):
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        cleaned = response_text.strip()
        
        # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë‹¨ìˆœí™” (ìŒì„± ì¶œë ¥ìš©)
        cleaned = re.sub(r'\*\*(.*?)\*\*', r'\1', cleaned)  # **êµµê²Œ** â†’ êµµê²Œ
        cleaned = re.sub(r'\*(.*?)\*', r'\1', cleaned)      # *ê¸°ìš¸ì„* â†’ ê¸°ìš¸ì„
        cleaned = re.sub(r'`(.*?)`', r'\1', cleaned)        # `ì½”ë“œ` â†’ ì½”ë“œ
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        
        return cleaned
    
    def summarize_text(self, text, target_sentences=2):
        """í…ìŠ¤íŠ¸ ìš”ì•½"""
        if len(text) <= app_config.max_summary_length:
            return text
        
        summary_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_sentences}ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. 
ìŒì„±ìœ¼ë¡œ ë“¤ì—ˆì„ ë•Œ í•µì‹¬ ë‚´ìš©ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”:

{text}

ìš”ì•½:"""
        
        try:
            response = self.model.generate_content(
                summary_prompt,
                generation_config={
                    "temperature": 0.3,  # ìš”ì•½ì€ ë” ì •í™•í•˜ê²Œ
                    "max_output_tokens": 200
                }
            )
            
            if response and response.text:
                return self._post_process_response(response.text)
            else:
                # ìš”ì•½ ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ ìë¥´ê¸°
                return text[:200] + "..."
                
        except Exception as e:
            print(f"ìš”ì•½ ì˜¤ë¥˜: {e}")
            return text[:200] + "..." if len(text) > 200 else text
    
    def analyze_sentiment(self, text):
        """ê°ì • ë¶„ì„"""
        sentiment_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ê³  ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”:

{text}

ê°ì • ë¶„ì„ ê²°ê³¼ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ì™€ ê°„ë‹¨í•œ ì´ìœ ):"""
        
        try:
            response = self.model.generate_content(
                sentiment_prompt,
                generation_config={
                    "temperature": 0.2,
                    "max_output_tokens": 100
                }
            )
            
            if response and response.text:
                return self._post_process_response(response.text)
            else:
                return "ê°ì • ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return "ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def generate_follow_up_questions(self, conversation_text, num_questions=3):
        """í›„ì† ì§ˆë¬¸ ìƒì„±"""
        followup_prompt = f"""ë‹¤ìŒ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í›„ì† ì§ˆë¬¸ {num_questions}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

{conversation_text}

í›„ì† ì§ˆë¬¸ë“¤:"""
        
        try:
            response = self.model.generate_content(
                followup_prompt,
                generation_config={
                    "temperature": 0.8,
                    "max_output_tokens": 300
                }
            )
            
            if response and response.text:
                questions = self._post_process_response(response.text)
                # ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
                question_list = [q.strip() for q in questions.split('\n') if q.strip()]
                return question_list[:num_questions]
            else:
                return []
                
        except Exception as e:
            print(f"í›„ì† ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return []
    
    def test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸...")
        
        if not api_keys.gemini_api_key:
            print("âŒ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return False
        
        try:
            test_response = self.generate_response("ì•ˆë…•í•˜ì„¸ìš”. ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
            
            if test_response and "ì£„ì†¡í•©ë‹ˆë‹¤" not in test_response:
                print("âœ… Gemini API ì—°ê²° ì„±ê³µ")
                print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response[:50]}...")
                return True
            else:
                print("âŒ Gemini API ì—°ê²° ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ Gemini API í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
            models = genai.list_models()
            available_models = []
            
            for model in models:
                if 'generateContent' in model.supported_generation_methods:
                    available_models.append(model.name)
            
            return {
                'current_model': self.model_name,
                'available_models': available_models,
                'generation_config': self.generation_config
            }
            
        except Exception as e:
            print(f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def change_model(self, model_name):
        """ëª¨ë¸ ë³€ê²½"""
        try:
            self.model = genai.GenerativeModel(model_name)
            self.model_name = model_name
            print(f"âœ… ëª¨ë¸ì´ {model_name}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë³€ê²½ ì˜¤ë¥˜: {e}")
            return False